task: vsr
dtype: "float32"
device: "cuda"

init:
input_size:
aux_ctc:

# normalize related
normalize:

# frontend related
frontend: conv3dresnet18
frontend_conf:
  activation_type: "swish"

# spec augment related
specaug:

# encoder related
encoder: simt_my_e_branchformer
encoder_conf:
  output_size: 256
  attention_heads: 4
  linear_units: 2048
  num_blocks: 12
  num_groups: 1
  cgmlp_linear_units: 2048
  cgmlp_conv_kernel: 31
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  attention_dropout_rate: 0.1
  attn_branch_drop_rate: 0.0
  input_layer: "linear"
  rel_pos_type: "latest"
  pos_enc_layer_type: "rel_pos"
  attention_layer_type: "rel_selfattn"
  positionwise_layer_type: "linear"
  ffn_activation_type: "swish"
  merge_method: "learned_ave"
  use_attn: true
  use_cgmlp: true
  macaron: true

# decoder related
decoder: simt_mlm
decoder_conf:
  attention_heads: 4
  linear_units: 2048
  num_blocks: 2
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  mha_attention_dropout_rate: 0.1

# ctc
ctc_conf:
  dropout_rate: 0.1
  ctc_type: "builtin"
  reduce: true

# model related
model: maskctc
model_conf:
  ctc_weight: 0.3
  interctc_weight: 0.0
  ignore_id: -1
  lsm_weight: 0.1
  length_normalized_loss: false
  report_cer: true
  report_wer: false
  sym_space: "‚ñÅ"
  sym_blank: "<blank>"

# inference related
inference_conf:
  maskctc_n_iterations: 10
  maskctc_threshold_probability: 0.99
  device: "cpu"

# token related
token_type: bpe
bpemodel: "./src/tokenizers/spm/256vocab/english.model"
token_list: "./src/tokenizers/spm/256vocab/english.token"

# training related
training_settings:
  nframes: 400
  optimizer: "adam"
  scheduler: "noam"
  batch_size: 16
  warmup_steps: 25000
  learning_rate: 0.0004
  noam_factor: 1.0
  accum_grad: 1
  grad_clip: -1.0
  epochs: 200
  average_epochs: 20
  use_amp: false
